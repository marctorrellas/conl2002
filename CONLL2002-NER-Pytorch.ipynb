{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 725 ms, sys: 272 ms, total: 997 ms\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(token, pos, tag)^N$ --> $(tokens, tags)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 205 ms, sys: 60.4 ms, total: 266 ms\n",
      "Wall time: 341 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from data_preparation import get_tokens_tags_from_sents\n",
    "train_tokens, train_tags = get_tokens_tags_from_sents(train_sents)\n",
    "val_tokens, val_tags = get_tokens_tags_from_sents(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Prepare mappings\n",
    "\n",
    "To train a neural network, we will use two mappings:\n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens for tokens will be:\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 0\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences. index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries\n",
    "from data_preparation import build_dict\n",
    "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight\n",
    "updates of the network are based on several sequences at every single time.\n",
    "The tricky part is that all sequences within a batch need to have the same\n",
    "length. So we will pad them with a special `<PAD>` token. It is also a good\n",
    "practice to provide RNN with sequence lengths, so it can skip computations\n",
    "for padding parts. We provide the batching function *batches_generator*\n",
    "readily available for you to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from data_preparation import batches_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1904b550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence, pad_packed_sequence\n",
    ")\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_dim, hidden_dim, vocab_size,\n",
    "                 tagset_size, padding_idx, verbose=False, bidirectional=False):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(\n",
    "            vocab_size, embedding_dim, padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "        self.tagset_size = tagset_size\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear((1+bidirectional)*hidden_dim, tagset_size)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, X, X_lens):\n",
    "        # embeddings\n",
    "        embeds = self.word_embeddings(X)\n",
    "        if self.verbose: print(f\"Embeds: {embeds.size()}\")\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be\n",
    "        # shown to the LSTM\n",
    "        embeds = pack_padded_sequence(embeds, X_lens.cpu().numpy(), batch_first=True)\n",
    "        # lstm\n",
    "        #lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # undo the packing operation\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        if self.verbose: print(f\"lstm_out: {lstm_out.size()}\")\n",
    "        # (batch_size, seq_len, hidden_dim) --> (batch_size * seq_len, hidden_dim)\n",
    "        s = lstm_out.contiguous().view(-1, lstm_out.shape[2])\n",
    "        # (batch_size * seq_len, hidden_dim) --> (batch_size * seq_len, tag_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        if self.verbose: print(f\"tag space: {tag_space.size()}\")\n",
    "        # normalize logits\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        if self.verbose: print(f\"tag scores: {tag_scores.size()}\")\n",
    "        return tag_scores\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_hat, y):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        return criterion(y_hat.view(-1, y_hat.size()[2]), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-MISC']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_score = list(tag2idx.keys())\n",
    "labels_to_score.remove('O')\n",
    "labels_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels_to_score,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import eval_model_for_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "VOCAB_SIZE = len(token2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "PADDING_IDX = token2idx[\"<PAD>\"]\n",
    "training_data = (train_tokens, train_tags)\n",
    "model = LSTMTagger(BATCH_SIZE, EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE,\n",
    "                   TAGSET_SIZE, PADDING_IDX, verbose=False, bidirectional=True)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-10 13:05:34 - START!\n",
      "2019-03-10 13:08:28 - avg epoch 1 train loss: 0.3591\n",
      "2019-03-10 13:11:14 - avg epoch 2 train loss: 0.1266\n",
      "2019-03-10 13:14:44 - avg epoch 3 train loss: 0.0665\n",
      "2019-03-10 13:17:36 - avg epoch 4 train loss: 0.0401\n",
      "2019-03-10 13:20:55 - avg epoch 5 train loss: 0.0272\n",
      "2019-03-10 13:20:55 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-10 13:24:11 - f1 score: 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.961     0.967     0.964      4913\n",
      "       I-LOC      0.979     0.950     0.964      1891\n",
      "      B-MISC      0.959     0.950     0.954      2173\n",
      "      I-MISC      0.961     0.979     0.970      3212\n",
      "       B-ORG      0.968     0.976     0.972      7390\n",
      "       I-ORG      0.980     0.973     0.976      4992\n",
      "       B-PER      0.989     0.994     0.991      4321\n",
      "       I-PER      0.994     0.993     0.994      3903\n",
      "\n",
      "   micro avg      0.974     0.976     0.975     32795\n",
      "   macro avg      0.974     0.973     0.973     32795\n",
      "weighted avg      0.974     0.976     0.975     32795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-10 13:24:13 - Loss: 1.587467181707325e-06\n",
      "2019-03-10 13:24:13 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-10 13:24:19 - f1 score: 0.719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.822     0.697     0.754      1084\n",
      "       I-LOC      0.798     0.597     0.683       325\n",
      "      B-MISC      0.625     0.546     0.583       339\n",
      "      I-MISC      0.621     0.558     0.588       557\n",
      "       B-ORG      0.743     0.823     0.781      1400\n",
      "       I-ORG      0.691     0.832     0.755      1104\n",
      "       B-PER      0.417     0.917     0.573       735\n",
      "       I-PER      0.813     0.864     0.838       634\n",
      "\n",
      "   micro avg      0.664     0.767     0.712      6178\n",
      "   macro avg      0.691     0.729     0.694      6178\n",
      "weighted avg      0.701     0.767     0.719      6178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-10 13:24:19 - Loss: 0.00013211334589868784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 28s, sys: 3min 9s, total: 34min 38s\n",
      "Wall time: 18min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print predictions before training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loader = batches_generator(\n",
    "        BATCH_SIZE, train_tokens, train_tags, token2idx, tag2idx, seed=epoch\n",
    "    )\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        # disabled for now\n",
    "        if (idx_batch + 1) % 970 == 0:\n",
    "            logger.info(\n",
    "                f'Epoch [{epoch + 1}/{EPOCHS}], '\n",
    "                f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % 5) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        train_loss.append(eval_model_for_set(\n",
    "            model, train_tokens, train_tags, token2idx, tag2idx, sorted_labels\n",
    "        ))\n",
    "        logger.info(f\"Loss: {train_loss[-1]}\")\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        val_loss.append(eval_model_for_set(\n",
    "            model, val_tokens, val_tags, token2idx, tag2idx, sorted_labels\n",
    "        ))\n",
    "        logger.info(f\"Loss: {val_loss[-1]}\")\n",
    "\n",
    "# print predictions after training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "#print(training_data[1][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really decent, given the simplicity of the model (it's just a BiLSTM with a dense layer afterwards). Lot of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Accuracy:\n",
    "* Dropout\n",
    "* Early stopping\n",
    "* Fine-tunning hyperparams: learning rate (https://www.jeremyjordan.me/nn-learning-rate/), embedding and hidden dimensions\n",
    "* Use trained embeddings\n",
    "* CRF / CNN\n",
    "\n",
    "Coding:\n",
    "* Use `DataLoader` from Pytorch rather than `batches_generator`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
