{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.94 s, sys: 164 ms, total: 3.1 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(token, pos, tag)^N$ --> $(tokens, tags)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 ms, sys: 58.6 ms, total: 278 ms\n",
      "Wall time: 338 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from data_preparation import get_tokens_tags_from_sents\n",
    "train_tokens, train_tags = get_tokens_tags_from_sents(train_sents)\n",
    "val_tokens, val_tags = get_tokens_tags_from_sents(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Prepare mappings\n",
    "\n",
    "To train a neural network, we will use two mappings:\n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens for tokens will be:\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 0\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences. index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries\n",
    "from data_preparation import build_dict\n",
    "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight\n",
    "updates of the network are based on several sequences at every single time.\n",
    "The tricky part is that all sequences within a batch need to have the same\n",
    "length. So we will pad them with a special `<PAD>` token. It is also a good\n",
    "practice to provide RNN with sequence lengths, so it can skip computations\n",
    "for padding parts. We provide the batching function *batches_generator*\n",
    "readily available for you to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from data_preparation import batches_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1c234790>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence, pad_packed_sequence\n",
    ")\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, seq_len, 1) --> (batch_size, seq_len, tag_dim)\n",
    "# the dimension = 1 is the one giving the token indexes\n",
    "from models import LSTMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, seq_len, tag_dim) --> (batch_size, seq_len, 1), selecting the best tag\n",
    "# sequence using Viterbi decoding\n",
    "# import from NCRF++\n",
    "from crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRFTagger(nn.Module):\n",
    "    # based on SeqLabel from NCRF++\n",
    "    # This is a wrapper to use the CRF after LSTMtagger\n",
    "    \n",
    "    def __init__(self, lstm_args, gpu=False):\n",
    "        super(LSTM_CRFTagger, self).__init__()\n",
    "        ## add two more labels for downlayer lstm, use original label size for CRF\n",
    "        tagset_size = lstm_args[\"tagset_size\"]\n",
    "        lstm_args[\"tagset_size\"] += 2\n",
    "        self.lstm = LSTMTagger(**lstm_args)\n",
    "        self.crf = CRF(tagset_size, gpu)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_mask(X_lens, batch_size, seq_len):\n",
    "        mask = Variable(torch.zeros((batch_size, seq_len))).byte()\n",
    "        for idx, X_len in enumerate(X_lens):\n",
    "            mask[idx, :X_len] = torch.Tensor([1]*int(X_len))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, X, X_lens, nbest=None):\n",
    "        tag_scores = self.lstm(X, X_lens)\n",
    "        batch_size, seq_len = X.size()\n",
    "        mask = self._get_mask(X_lens, batch_size, seq_len)\n",
    "        # use this for training\n",
    "        if not nbest:\n",
    "            _, tag_seq = self.crf._viterbi_decode(tag_scores, mask)\n",
    "            return tag_seq, mask\n",
    "        # use this for testing\n",
    "        # TODO: check speed of _viterbi_decode_nbest vs _viterbi_decode. If nbest=1\n",
    "        # maybe it's better to use _viterbi_decode\n",
    "        else:\n",
    "            scores, tag_seq = self.crf._viterbi_decode_nbest(tag_scores, mask, nbest)\n",
    "            return scores, tag_seq\n",
    "        \n",
    "    def loss(self, tag_scores, y_hat, mask):\n",
    "        total_loss = self.crf.neg_log_likelihood_loss(tag_scores, mask, y_hat)\n",
    "        return total_loss / y.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-MISC']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_score = list(tag2idx.keys())\n",
    "labels_to_score.remove('O')\n",
    "labels_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels_to_score,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import eval_model_for_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "lstm_args = {\n",
    "    \"embedding_dim\": 50, \n",
    "    \"hidden_dim\": 20, \n",
    "    \"vocab_size\": len(token2idx), \n",
    "    \"tagset_size\": len(tag2idx), \n",
    "    \"padding_idx\": token2idx[\"<PAD>\"], \n",
    "    \"verbose\": False, \n",
    "    \"bidirectional\": False\n",
    "}\n",
    "model = LSTM_CRFTagger(lstm_args)\n",
    "LEARNING_RATE = 0.005\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-10 23:52:34 - START!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-1c099ce7341a>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y, y_hat, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PycharmProjects/notebooks/CONLL2002/crf.py\u001b[0m in \u001b[0;36mneg_log_likelihood_loss\u001b[0;34m(self, feats, mask, tags)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mneg_log_likelihood_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# nonegative log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mforward_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_PZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mgold_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_score_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PycharmProjects/notebooks/CONLL2002/crf.py\u001b[0m in \u001b[0;36m_calculate_PZ\u001b[0;34m(self, feats, mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mtag_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtag_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print predictions before training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loader = batches_generator(\n",
    "        BATCH_SIZE, train_tokens, train_tags, token2idx, tag2idx, seed=epoch\n",
    "    )\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_seqs, mask = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_seqs, batch_tags, mask)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        # disabled for now\n",
    "        if (idx_batch + 1) % 970 == 0:\n",
    "            logger.info(\n",
    "                f'Epoch [{epoch + 1}/{EPOCHS}], '\n",
    "                f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % 5) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        train_loss.append(eval_model_for_set(\n",
    "            model, train_tokens, train_tags, token2idx, tag2idx, sorted_labels\n",
    "        ))\n",
    "        logger.info(f\"Loss: {train_loss[-1]}\")\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        val_loss.append(eval_model_for_set(\n",
    "            model, val_tokens, val_tags, token2idx, tag2idx, sorted_labels\n",
    "        ))\n",
    "        logger.info(f\"Loss: {val_loss[-1]}\")\n",
    "\n",
    "# print predictions after training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "#print(training_data[1][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Accuracy:\n",
    "* Dropout\n",
    "* Early stopping\n",
    "* Fine-tunning hyperparams: learning rate (https://www.jeremyjordan.me/nn-learning-rate/), embedding and hidden dimensions\n",
    "* Use trained embeddings / hand-crafted features\n",
    "* CNN\n",
    "\n",
    "Speed:\n",
    "* _viterbi_decode_nbest vs _viterbi_decode when nbest=1\n",
    "\n",
    "Coding:\n",
    "* Use `DataLoader` from Pytorch rather than `batches_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(training_data, i, model, word2idx, idx2tag):\n",
    "    pass\n",
    "    # Note that element i,j of tag_scores is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    # with torch.no_grad():\n",
    "    #     seq = training_data[0][i]\n",
    "    #     labs = training_data[1][i]\n",
    "    #     inputs = prepare_sequence(seq, word2idx)\n",
    "    #     tag_scores = model(inputs.view(1, len(inputs)),\n",
    "    #                        torch.tensor([len(seq)]))\n",
    "    #     tags = np.vectorize(idx2tag.get)(torch.argmax(tag_scores, dim=2).data.numpy())\n",
    "    #     print(seq)\n",
    "    #     print()\n",
    "    #     print(tags)\n",
    "    #     print()\n",
    "    #     print(len(seq), tag_scores.size(), tags.shape)\n",
    "    #     print()\n",
    "    #     print(training_data[1][i])\n",
    "    #     print(training_data[1][i] == tags)\n",
    "#print_example(training_data, 79, model, token2idx, idx2tag)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
