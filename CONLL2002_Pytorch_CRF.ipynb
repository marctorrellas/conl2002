{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(token, pos, tag)^N$ --> $(tokens, tags)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from data_preparation import ConLL2002DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = ConLL2002DataSet(\"esp.train\").get_tokens_tags_from_sents()\n",
    "val_tokens, val_tags = ConLL2002DataSet(\"esp.testb\").get_tokens_tags_from_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you\n",
    "can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare mappings\n",
    "\n",
    "A neural network needs to work with word indices, not next. Then, we need to learn\n",
    "the vocabulary of tokens and tags. This is accomplished with the Vectorizer, and then\n",
    "used to transform the datasets into VectorizedDataset objects\n",
    "\n",
    "Some special tokens in the vocabulary:\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of\n",
    " sentences. index = 0\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 1\n",
    " - `<START>` index = 2 (not used here)\n",
    " - `<END>` index = 3 (not used here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(use_start_end=False, use_pad=True)\n",
    "vectorizer.fit(train_tokens, train_tags)\n",
    "train_data = vectorizer.transform(train_tokens, train_tags)\n",
    "val_data = vectorizer.transform(val_tokens, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens[0])\n",
    "print(train_data.input[0])\n",
    "vectorizer.map_sequence_back(vectorizer.word_vocab, train_data.input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tags[0])\n",
    "print(train_data.target[0])\n",
    "vectorizer.map_sequence_back(vectorizer.tag_vocab, train_data.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight\n",
    "updates of the network are based on several sequences at every single time.\n",
    "The tricky part is that all sequences within a batch need to have the same\n",
    "length. So we will pad them with a special `<PAD>` token. It is also a good\n",
    "practice to provide RNN with sequence lengths, so it can skip computations\n",
    "for padding parts. We provide the batching function *batches_generator*\n",
    "readily available for you to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, seq_len, 1) --> (batch_size, seq_len, tag_dim)\n",
    "# the dimension = 1 is the one giving the token indexes\n",
    "from models import LSTMTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, seq_len, tag_dim) --> (batch_size, seq_len, 1), selecting the best tag\n",
    "# sequence using Viterbi decoding\n",
    "# import from NCRF++\n",
    "# from ncrfpp import CRF\n",
    "from torch.autograd import Variable\n",
    "from allennlp.modules import ConditionalRandomField\n",
    "from allennlp.models import CrfTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRFTagger(nn.Module):\n",
    "    # based on CRFtagger from AllenNLP\n",
    "    # This is a wrapper to use the CRF after LSTMtagger, so the flow is:\n",
    "    # embedding -- lstm -- hidden2tag (dense layer) -- CRF\n",
    "\n",
    "    def __init__(self, lstm_args):\n",
    "        super(LSTM_CRFTagger, self).__init__()\n",
    "        self.lstm = LSTMTagger(**lstm_args)\n",
    "        self.crf = ConditionalRandomField(\n",
    "            lstm_args[\"tagset_size\"], include_start_end_transitions=True\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_mask(X_lens, batch_size, seq_len):\n",
    "        mask = Variable(torch.zeros((batch_size, seq_len))).byte()\n",
    "        for idx, X_len in enumerate(X_lens):\n",
    "            mask[idx, :X_len] = torch.ones(X_len)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input, input_lens):\n",
    "        logits = self.lstm.forward(input, input_lens, apply_softmax=False)\n",
    "        batch_size, seq_len, _ = logits.size()\n",
    "        mask = __class__._get_mask(input_lens, batch_size, seq_len)\n",
    "        return logits, mask\n",
    "\n",
    "    def loss(self, logits, mask, target):\n",
    "        \"\"\"Use negative log-likelihood as loss\"\"\"\n",
    "        log_likelihood = self.crf(logits, target, mask)\n",
    "        return -log_likelihood\n",
    "\n",
    "    def decode(self, logits, mask):\n",
    "        \"\"\"Return most probable sequence using Viterbi\"\"\"\n",
    "        best_paths = self.crf.viterbi_tags(logits, mask)\n",
    "        # Just get the tags and ignore the score\n",
    "        return [best_sequence for best_sequence, score in best_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from evaluation import eval_model_for_set\n",
    "from torch.utils.data import DataLoader\n",
    "from data_preparation import pad_and_sort_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "PRINT_EVERY_NBATCHES = 100\n",
    "PRINT_EVERY_NEPOCHS = 1\n",
    "lstm_args = {\n",
    "    \"embedding_dim\": 200,\n",
    "    \"hidden_dim\": 200,\n",
    "    \"vocab_size\": len(vectorizer.word_vocab),\n",
    "    \"tagset_size\": len(vectorizer.tag_vocab),\n",
    "    \"bidirectional\": True\n",
    "}\n",
    "model = LSTM_CRFTagger(lstm_args)\n",
    "LEARNING_RATE = 0.005\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    # TODO: review how to set the seed\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_and_sort_batch\n",
    "    )\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores, mask = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_scores, mask, batch_tags)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        # disabled for now\n",
    "        if (idx_batch + 1) % PRINT_EVERY_NBATCHES == 0:\n",
    "            logger.info(\n",
    "                f\"Epoch [{epoch + 1}/{EPOCHS}], \"\n",
    "                f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % PRINT_EVERY_NEPOCHS) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        train_loss.append(eval_model_for_set(model, train_data, vectorizer, True))\n",
    "        logger.info(f\"Loss: {train_loss[-1]}\")\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        val_loss.append(eval_model_for_set(model, val_data, vectorizer, True))\n",
    "        logger.info(f\"Loss: {val_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "* Dropout\n",
    "* Early stopping\n",
    "* Fine-tunning hyperparams: learning rate (https://www.jeremyjordan.me/nn-learning-rate/), embedding and hidden dimensions\n",
    "* Use trained embeddings / hand-crafted features\n",
    "* CNN\n",
    "\n",
    "Speed:\n",
    "* _viterbi_decode_nbest vs _viterbi_decode when nbest=1\n",
    "\n",
    "Coding:\n",
    "* Clean NCRF++ implementation, probably more efficient"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
