{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 577 ms, sys: 204 ms, total: 780 ms\n",
      "Wall time: 976 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(token, pos, tag)^N$ --> $(tokens, tags)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 211 ms, sys: 61.2 ms, total: 272 ms\n",
      "Wall time: 354 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from data_preparation2 import ConLL2002DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, train_tags = ConLL2002DataSet(\"esp.train\").get_tokens_tags_from_sents()\n",
    "val_tokens, val_tags = ConLL2002DataSet(\"esp.testb\").get_tokens_tags_from_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare mappings\n",
    "\n",
    "A neural network needs to work with word indices, not next. Then, we need to learn\n",
    "the vocabulary of tokens and tags. This is accomplished with the Vectorizer, and then\n",
    "used to transform the datasets into VectorizedDataset objects\n",
    "\n",
    "Some special tokens in the vocabulary:\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of\n",
    " sentences. index = 0\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 1\n",
    " - `<START>` index = 2 (not used here)\n",
    " - `<END>` index = 3 (not used here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation2 import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(use_start_end=False, use_pad=True)\n",
    "vectorizer.fit(train_tokens, train_tags)\n",
    "train_data = vectorizer.transform(train_tokens, train_tags)\n",
    "val_data = vectorizer.transform(val_tokens, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE', ')', '.')\n",
      "tensor([ 2,  3,  4,  5,  6,  7,  8,  3,  9,  5, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Melbourne', '(', 'Australia', ')', ',', '25', 'may', '(', 'EFE',\n",
       "       ')', '.'], dtype='<U9')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_tokens[0])\n",
    "print(train_data.input[0])\n",
    "vectorizer.map_sequence_back(vectorizer.word_vocab, train_data.input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O')\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_tags[0])\n",
    "print(train_data.target[0])\n",
    "vectorizer.map_sequence_back(vectorizer.tag_vocab, train_data.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight\n",
    "updates of the network are based on several sequences at every single time.\n",
    "The tricky part is that all sequences within a batch need to have the same\n",
    "length. So we will pad them with a special `<PAD>` token. It is also a good\n",
    "practice to provide RNN with sequence lengths, so it can skip computations\n",
    "for padding parts. We provide the batching function *batches_generator*\n",
    "readily available for you to save time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    \"\"\"\n",
    "    This class will define the following three blocks:\n",
    "    1. Embedding layer: from word index to embedding\n",
    "    2. (Bi)LSTM: from embedding to a representation of dimension hidden_dim\n",
    "    3. Hidden2tag: a dense layer from hidden_dim to the tag space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        vocab_size,\n",
    "        tagset_size,\n",
    "        verbose=False,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True, bidirectional=bidirectional\n",
    "        )\n",
    "        self.tagset_size = tagset_size\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear((1 + bidirectional) * hidden_dim, tagset_size)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, X, X_lens):\n",
    "        # embeddings\n",
    "        embeds = self.word_embeddings(X)\n",
    "        if self.verbose:\n",
    "            print(f\"Embeds: {embeds.size()}\")\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be\n",
    "        # shown to the LSTM\n",
    "        embeds = pack_padded_sequence(embeds, X_lens.cpu().numpy(), batch_first=True)\n",
    "        # lstm\n",
    "        # lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # undo the packing operation\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        if self.verbose:\n",
    "            print(f\"lstm_out: {lstm_out.size()}\")\n",
    "        #  (batch_size * seq_len, hidden_dim) --> (batch_size * seq_len, tag_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        if self.verbose:\n",
    "            print(f\"tag space: {tag_space.size()}\")\n",
    "        # normalize logits\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        if self.verbose:\n",
    "            print(f\"tag scores: {tag_scores.size()}\")\n",
    "        return tag_scores\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        return criterion(y_hat.view(-1, y_hat.size()[2]), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels_to_score = list(vectorizer.tag_vocab.keys())\n",
    "labels_to_score.remove(\"O\")\n",
    "labels_to_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "group B and I results\n",
    "sorted_labels = sorted(labels_to_score, key=lambda name: (name[1:], name[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import eval_model_for_set\n",
    "from torch.utils.data import DataLoader\n",
    "from data_preparation2 import pad_and_sort_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "VOCAB_SIZE = len(vectorizer.word_vocab)\n",
    "TAGSET_SIZE = len(vectorizer.tag_vocab)\n",
    "PADDING_IDX = 0\n",
    "PRINT_EVERY_NBATCHES = 100\n",
    "PRINT_EVERY_NEPOCHS = 1\n",
    "model = LSTMTagger(\n",
    "    EMBEDDING_DIM,\n",
    "    HIDDEN_DIM,\n",
    "    VOCAB_SIZE,\n",
    "    TAGSET_SIZE,\n",
    "    verbose=False,\n",
    "    bidirectional=True,\n",
    ")\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print predictions before training\n",
    "# print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS):\n",
    "    # TODO: review how to set the seed\n",
    "    train_loader = DataLoader(\n",
    "        train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_and_sort_batch\n",
    "    )\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        # disabled for now\n",
    "        if (idx_batch + 1) % PRINT_EVERY_NBATCHES == 0:\n",
    "            logger.info(\n",
    "                f\"Epoch [{epoch + 1}/{EPOCHS}], \"\n",
    "                f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % PRINT_EVERY_NEPOCHS) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        train_loss.append(eval_model_for_set(model, train_data, vectorizer))\n",
    "        logger.info(f\"Loss: {train_loss[-1]}\")\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        val_loss.append(eval_model_for_set(model, val_data, vectorizer))\n",
    "        logger.info(f\"Loss: {val_loss[-1]}\")\n",
    "\n",
    "# print predictions after training\n",
    "# print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "# print(training_data[1][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really decent, given the simplicity of the model (it's just a BiLSTM with a dense layer afterwards). Lot of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Accuracy:\n",
    "* Dropout\n",
    "* Early stopping\n",
    "* Fine-tunning hyperparams: learning rate (https://www.jeremyjordan.me/nn-learning-rate/), embedding and hidden dimensions\n",
    "* Use trained embeddings\n",
    "* CRF / CNN\n",
    "\n",
    "Coding:\n",
    "* Use `DataLoader` from Pytorch rather than `batches_generator`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
